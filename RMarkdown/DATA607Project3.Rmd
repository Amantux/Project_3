---
title: "DATA607 Project 3"
author: "MEAO - A. Moyse, A. Elsaeyed, C. Alvarez, P. O'Flaherty"
date: '2022-03-22'
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: true
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

<!--
Additional pieces:
Visio data output file?

Parameterize different portions of it and throw it through cognitive services on Azure?

Text files, pull whole page down to the character, per section and flatten

different tables: relationship or non relationship

mongo databricks PKID

Keyword selection algorithm
-->

<!--
Project – Data Science Skills
This is a project for your entire class section to work on together, since being able to work effectively on a virtual team is a
key “soft skill” for data scientists. Please note especially the requirement about making a presentation during our first
meetup after the project is due.

W. Edwards Deming said, “In God we trust, all others must bring data.” Please use data to answer the question,
“Which are the most valued data science skills?” Consider your work as an exploration; there is not necessarily a “right
answer.”
Grading rubric:
 You will need to determine what tool(s) you’ll use as a group to effectively collaborate, share code and any
project documentation (such as motivation, approach, findings).
 You will have to determine what data to collect, where the data can be found, and how to load it.
 The data that you decide to collect should reside in a relational database, in a set of normalized tables.
 You should perform any needed tidying, transformations, and exploratory data analysis in R.
 Your deliverable should include all code, results, and documentation of your motivation, approach, and findings.
 As a group, you should appoint (at least) three people to lead parts of the presentation.
 While you are strongly encouraged (and will hopefully find it fun) to try out statistics and data models, your
grade will not be affected by the statistical analysis and modeling performed (since this is a semester one
course on Data Acquisition and Management).
 Every student must be prepared to explain how the data was collected, loaded, transformed, tidied, and
analyzed for outliers, etc. in our Meetup. This is the only way I’ll have to determine that everyone actively
participated in the process, so you need to hold yourself responsible for understanding what your class-size
team did! If you are unable to attend the meet up, then you need to either present to me one-on-one before
the meetup presentation, or post a 3 to 5 minute video (e.g. on YouTube) explaining the process. Individual
students will not be responsible for explaining any forays into statistical analysis, modeling, data mining,
regression, decision trees, etc.
You are encouraged to start early, ask many questions, actively post on the provided discussion forum, etc.

One example graph: Top Forest Ranger Skills (based on number of resumes with specified skills). You are
encouraged to come up with your own approach that may use different kinds of data sources.
-->

<br>

* * *

# Most Valued Data Science Skills

## Getting Started

### Instructions

Use data to answer the question, “Which are the most valued data science skills?”

<br>

* * *

### Proposed Implementation

Our goal is to use multiple data sets from Kaggle to:

 + create a rough association between job titles and pay bands
 + identify skill sets in job listings using a codex
 + assign a value to individual skill sets using the above

A flex for the project would be to create a shiny app with embedded salary predictors based on:
 + location
 + 

<br>

* * *

### Libraries and tech

**Here we load our libraries and explain the different tech used.**

***LOAD LIBRARY CHUNK***

Fuzzy wuzzy

<br>

* * *

## Data

We transform the data from a one to very many creating a really wide table out of the data.

The fifth table is the first four views unioned together.

Subsecting and mapping the database

Create views that are queries that are pre-cached and preprocessed

Data Collection:
 + Job title
 + skill set
 + pay
 + location


***NOTE: Transition to next section***
Because there's a lot of job listings without pay we need to create a pay predictor based on Job Title.

***NOTE: anything interesting to say about location being a factor in predicting pay but why it's not necessary to include here?)***

<br>

* * *

## Pay bands



***ENTER AHMED PIECE***

<br>

* * *

## Skills

Originally we wanted to pursue two codexes, one for soft skills that are more likely to be multiple word patterns, and a second for hard skills which are more likely to be single word patterns.

Ultimately by using the fuzzy wuzzy library we didn't need to use different methods for single versus multiple word patterns and so we collapsed the codex into one.

We started with a premade list of 100 different skills and our final was created by reading the job listings.

***ENTER CARLOS PIECE***

<br>

* * *

## Analysis

***NOTE: This may be used instead of the predictor algorithm section.  Depending on how we answer the question.***

My understanding is this is where we assign a value to individual skills based on the above and we don't need the predictor algorithm section.  But there was a lot of talk about the predictor algorithm.

<br>

* * *

## Predictor Algorithm

***NOTE: This may replace the Analysis section if we go the classifier route.***

How does the classifier work?

Creating a prediction algorithm?

Predictor-based algorithm, I have these parameters and this is what we're extracting out and key it in.

* * *

## Concluding Remarks

### Decisions made

**Data**
Initially we wanted to scrape job listings from job boards like Indeed and LinkedIn; However we ran into issues with being able to collect data at scale.  The kaggle datasets allowed us to pursue our lines of analysis as a proof of concept.  The natural progression of the project would be to duplicate it using current, scraped job listings.

**Predictors**
Because we have so much data from the job listings, an interesting project would be to create salary predictors based on location, job title, and required skills.

**neo4j vs. MongoDB**

### Resources

 + Here is a tutorial for web scraping in  [RSelenium](http://joshuamccrain.com/tutorials/web_scraping_R_selenium.html)
 + [Trello](https://trello.com) is a way to collective track and prioritize elements of a project
 + [neo4j](https://neo4j.com/developer/get-started/) is database system which organizes data based on association
 + [docker](https://docs.docker.com/compose/gettingstarted/) is a way to create instances of an environment to run your code.  It's a best in practice because any docking container you create would be identical across computers.
 
* * *


