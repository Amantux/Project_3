Hi, Data Engineer (Azure Delta Lake and Databricks) Location is Philly, PA Duration is at least 9 months Job Summary As part of Data Engineering team, you will be architecting and delivering highly scalable, high performance data integration and transformation platforms. The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills. You will be using core cloud data warehouse tools, hadoop, spark, events streaming platforms and other data management related technologies. You will also engage in requirements and solution concept development, requiring strong analytic and communication skills. Responsibilities Function as the solution lead for building the data pipelines to support the development enablement of Information Supply Chains within our client organizations ndash this could include building (1) data provisioning frameworks, (2) data integration into data warehouse, data marts and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores. Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration. Develop overall design and determine division of labor across various architectural components Deploy and customize Client Standard Architecture components Mentor client personnel. Train clients on the Client Integration Methodology and related supplemental solutions Provide feedback and enhance Client intellectual property related to data management technology deployments Assist in development of task plans including schedule and effort estimation Skills and Qualifications 1+ year experience working with Azure analytical stack Experience with building analytical solutions on Azure Synapse Experience with building Delta Lake is required (Databricks) Strong experience building and deploying data management solutions on DataBricks is required Experience building high-performance, and scalable distributed systems Experience in ETL and ELT workflow management Continuous Data Movement Streaming Messaging Experience with related technologies ex Spark streaming or other message brokers like Kafka is a PLUS 3+ yearsrsquo experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing) 3+ yearsrsquo experience in a software engineering, leveraging Java, Python, Scala, etc. 2+ yearsrsquo advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns 2+ yearsrsquo experience with distributed NoSQL databases and event brokers (Apache Cassandra, , Graph databases, Document Store databases) Sincerely, HR Manager nFolks Data Solutions LLC Phone 425-999-4933 Email arun(AT)nfolksdata.com
