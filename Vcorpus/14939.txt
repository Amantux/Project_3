Location: Phoenix (AZ)
Location: Pittsburgh (PA)

Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we've been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers.

The Data Science team lies at the foundation of Zoom's success - you'll be working cross-functionally with teams of engineers, scientists, marketers, and product professionals on some of the most critical projects in the company - whether it's exploratory research to predict user behavior, or running experiments to optimize untapped areas of growth, or developing machine learning models that deliver "happiness" to our users more consistently and at scale. If you are passionate about data engineering and looking to join a fun and fast-moving team, we'd love to meet you! Our team is taking Zoom's data culture to the next level by integrating predictive models into our infrastructure, and we are looking for someone like you to help us get there! This role is based in Pittsburgh or Phoenix.

Responsibilities
Own and optimize Zoom's data architecture to address the data needs of our rapidly-growing business
Join a group of passionate people committed to delivering "happiness" to our users and to each other
Partner with data scientists, sales, marketing, operation, and product teams to build and deploy machine learning models that unlock growth
Build custom integrations between cloud-based systems using APIs
Write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL
Architect, build, and launch new data models that provide intuitive analytics to the team
Build data expertise and own data quality for the pipelines you create
Requirements
Three or more years of relevant software engineering experience (Python, Scala and Java) in a data-focused role
Passion for creating data infrastructure technologies from scratch using the right tools for the job
A knack for writing, clean, readable, maintainable code
Comfort with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
Expertise in building out data pipelines, efficient ETL design, implementation, and maintenance
Experience with AWS tools
Proven track-record of solving complex data processing and storage challenges through scalable, fault-tolerant architecture
