This role will start out as 100 remote until clients start opening up to onsite work in the greater DTLA area. Some of the technologies used by various clients include Python Scala Java AWS Glue AWS Redshift AWS EMR AWS Data Pipeline AWS Kinesis GCP Dataflow GCP BigQuery Looker Snowflake Redshift Spectrum Builds and maintains distributed systems, services and data pipelines. The role is focused on working on complex distributed systems, the performance of these systems and the ongoing optimization in service of our core business. This role is required to write highly scalable and fault tolerant code Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark on Databricks, Apache Airflow and AWS lsquobig datarsquo technologies like AWS Glue, AWS Athena. Solid experience in SQL development Solid experience with Python, Spark, shell scripting Extensive ETL development experience with large-scale DBS or big data systems such as Redshift, Snowflake, Databricks, etc. Experience working with reporting tools such as Tableau or Looker Extensive experience with design development of relational databases and data warehouses Ability to look at solutions unconventionally and explore opportunities and devise innovative solutions
