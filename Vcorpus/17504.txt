Job Description
Not ready to apply? Message with the recruiter of this job to learn more:
https://strike.chat/s/schedule-chat/bcg_platinion/data_engineering/e677c?src=job-board
At BCG Platinion, we are thinkers, makers, and doers who love solving hard problems. We aren’t afraid to roll up our sleeves and dive into the weeds of complexity—in fact, that’s our job. When companies are tasked with understanding and managing their data, we come in and develop the products, tools, and platforms they need to harness that information for real impact.

(Senior) Data Engineers at BCG Platinion are:
Iterative. They are excited to prototype at all levels of fidelity—and have the humility to walk away from ideas when they fail.
Collaborative. They have the ability and enthusiasm to work with researchers, engineers, business consultants, and other designers who will challenge and support one another.
Comfortable with ambiguity. They know projects and businesses move fast. That means the path forward isn’t always well-defined. They are comfortable and collaborative through our process.
Interdisciplinary. They deliver data products for digital solutions, deploy analytical models into production, fix existing data platforms, or coach and enable other teams in best practices depending on need.
You’re Good At:
Working with a diverse set of clients across domains and industries
Implementing data orchestration pipelines, data sourcing, cleansing, and augmentation and quality

control processes
Deploying machine learning models in production
Supporting data architects in designing data architectures
Assisting in mentoring data engineers to further their personal and professional growth
Supporting project management operations of a project
Translating business needs into solutions
Contributing to overall solution, integration, and enterprise architecture
You’ll Bring:
2+ years of experience working on large scale, full lifecycle data implementation projects
BS/BA in data engineering, software engineering, data science, computer science, applied mathematics, or equivalent experience
2+ years professional development experience with some of the AWS/Azure/GCP data stack: S3, Redshift, AWS glue, EMR, Azure Data Warehouse, Azure Blob Store, Google Big Query
A deep knowledge of performant SQL and understanding of relational database technology
Hands-on RDBMS experience (data modeling, analysis, programming, stored procedures)
Expertise in developing ETL/ELT workflows with one or more of the following: Python, Scala, Java
Deployment of data pipelines in the Cloud in at least AWS, Azure, or GCP
A deep understanding of relational and warehousing database technology,

working with at least one of the major databases platforms (Oracle, SQLServer, Teradata, MySQL, Postgres)
Additional consideration to candidates who possess some of the following criteria:
Experience working with Big Data technologies such as Spark, Hive, Impala, Druid, or Presto
A solid foundation in data structures, algorithms, and OO Design with fundamentally strong programming skills
Proven success working in and promoting a rapidly changing, collaborative, and iterative product development environment
Strong interpersonal and analytical skills
Intellectual curiosity and an ability to execute projects
An understanding of “big picture” business requirements that drive architecture

and design decisions
DevOps and DataOps skills including “infrastructure as code” systems like

CloudFormation or Terraform
Data system performance tuning
Implementation of predictive analytics and machine learning models (MLlib,

scikit-learn, etc)
Willingness to travel around the globe to work with clients and BCG teams. At

times, this role involves significant travel to client sites. The amount of travel will depend on client needs and nature of projects
Not ready to apply? Message with the recruiter of this job to learn more:
https://strike.chat/s/schedule-chat/bcg_platinion/data_engineering/e677c?src=job-board
