Analyze and understand data sources & APIs
Design and Develop methods to connect & collect data from different data sources
Design and Develop methods to filter/cleanse the data
Design and Develop SQL , Hive queries, APIs to extract data from the store
Work closely with data Scientists to ensure the source data is aggregated and cleansed
Work with product managers to understand the business objectives
Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows
Work with DevOps to build automated data pipelines
Total Experience Required
4 years <10>
The candidate should have performed client facing roles and possess excellent communication skills
Business Domain knowledge: Finance & banking systems, Fraud, Payments
Required Technical Skills
Big Data-Hadoop, NoSQL, Hive, Apache Spark
Python
Java & REST
GIT and Version Control
Desirable Technical Skills
Familiarity with HTTP and invoking web-APIs
Exposure to machine learning engineering
Exposure to NLP and text processing
Experience with pipelines, job scheduling and workflow management
Personal Skills
Experienced in managing work with distributed teams
Experience working in SCRUM methodology
Proven sense of high accountability and self-drive to take on and see through big challenges
Confident, takes ownership, willingness to get the job done
Excellent verbal communications and cross group collaboration skills
Job Type: Contract
Schedule:
Monday to Friday
Experience:
Hive: 4 years (Required)
Apache Spark: 4 years (Required)
REST: 1 year (Required)
NoSQL: 2 years (Required)
Java: 1 year (Required)
Git: 1 year (Required)
SQL: 1 year (Required)
Big Data-Hadoop: 4 years (Required)
Python: 2 years (Required)
Contract Length:
5 - 6 months
Work Remotely:
Temporarily due to COVID-19
