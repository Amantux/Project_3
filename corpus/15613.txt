Data Engineer (AWS, Snowflake, Batch ETL tool) 12+ Months Phoenix, AZ San Antonio, TX Skills and Qualifications Data Engineer with Bachelorrsquos Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required 1+ year experience with Snowflake database AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift) Experience in ETL and ELT workflow management Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline Experience building internal cloud to cloud integrations is ideal Experience with streaming related technologies ex Spark streaming or other message brokers like Kafka is a plus 3+ years of Data Management Experience 3+ years of batch ETL tool experience (DataStage Informatica Talend) 3+ yearsrsquo experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing) 2+ yearsrsquo experience with Hadoop Ecosystem (HDFSS3, Hive, Spark) 2+ yearsrsquo experience in a software engineering, leveraging Java, Python, Scala, etc. 2+ yearsrsquo advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns 2+ yearsrsquo experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases) Experience in the financial services, banking and or Insurance industries is a nice to have
