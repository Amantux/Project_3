Employees at The RealReal are dedicated, collaborative and innovative, and we're looking for exceptional talent to join our team. Build your career with us and enjoy 401K matching, health, dental and vision insurance, commuter flex spending, healthcare flex spending, generous PTO, a mother's room, and flexible work hours! As a Data Scientist at The RealReal you'll drive our automation, personalization and data pipeline initiatives. You will own critical systems throughout the platform and be responsible for their adding new, valuable features, and ensuring that these systems perform correctly. This position is based in San Francisco, CA and will report to the Director of Data Engineering. DUTIES & RESPONSIBILITIES Design and test machine learning models to create highly accurate systems used in operations Write production quality code Take ownership of key components, ensuring that they meet the needs of the business. Collaborate with senior management, product management, and other engineers in the development of product Mentor team members to build the company's overall expertise Work to make The RealReal an innovator in the space by bringing passion and new ideas to work every day REQUIREMENTS A quantitative degree 2-4+ years experience in a similar role 2-4+ years experience in building and evaluating machine learning & deep learning models 2-4+ years experience putting machine learning models into production Fluency in Python & SQL High degree of familiarity with: Sklearn, TensorFlow/Keras, Pytorch An in depth knowledge or project you can showcase your skills Comfortable working in an environment where innovation and change are the norm A passion for building great products ADDITIONAL REQUIREMENTS If computer vision: Experience with classical computer vision techniques (image registration, thresholding, camera calibration, keypoint detection, etc) Experience with deep learning applied to computer vision (image classification, object detection, semantic segmentation, GANs, etc) If data engineering: Experience building sustainable and scalable ETL pipelines Experience with different types of databases (Key-Value, Document, Relational, MPP, etc) Deep experience with cloud architectures (AWS/GCP) using serverless designs and event driven architectures Experience with Kafka/Kinesis/PubSub event-based designs Experience writing and reading from APIs
