Client is seeking 2 Hadoop Data Engineers to help manage an offer personalization engine built with Python/PySpark on AWS EMR clusters. Responsibilities: Perform data engineering tasks using PySpark to aggregate multiple data sources into a centralized repository on AWS Working closely with Data Science team to maintain and improve machine learning models and PySpark based personalization/assignment/measurement tools Develop new features and augment tools based on business requirements Technical Ownership Skills: Data Engineering & data flow pipelines Handling unstructured data Software/application engineering Continuous Integration Knowledge of data science techniques including supervised and unsupervised learning Knowledge of A/B testing/Experimental design Business Ownership skills: Data ownership Iteration & unit-testing Hypothesis testing & Inductive thinking Experience communicating technical/analytics solutions to senior-level business stakeholders Languages/Tools: Advanced Python knowledge (pandas, numpy) Spark - using pySpark SQL (analytic functions, subqueries, etc) Git & Github Hadoop: AWS EMR (Debian), Hive Jenkins (Continuous Integration) Tableau/Excel
